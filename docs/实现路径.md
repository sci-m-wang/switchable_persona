## üîß Overall Pipeline Structure

1. **Data Collection**
2. **Data Processing & Personality Layer Extraction**
3. **Model Training & Injection**
4. **Evaluation: Task Design & Metrics**
5. **Implementation Timeline & Task Assignment**

---

## 1. üì° Data Collection

| Source Type     | Description                                   | Example Sources                                     |
| --------------- | --------------------------------------------- | --------------------------------------------------- |
| **Podcast**     | Audio/video shows with opinionated discussion | ‚ÄúThe Daily,‚Äù ‚ÄúLex Fridman,‚Äù ‚ÄúHard Fork‚Äù             |
| **Twitter**     | Short posts reflecting emotion, stance, style | Threads by tech influencers or political figures    |
| **TV Dialogue** | Character-consistent fictional conversations  | ‚ÄúFriends,‚Äù ‚ÄúThe Office,‚Äù OpenSubtitles, DailyDialog |

---

## 2. üß† Data Processing & Three-Layer Personality Extraction

### üéØ Goal: Construct (Context, Personality, Response) tuples

### 2.1 Layer Definitions & Extraction Methods

| Layer                 | Attributes Extracted                           | Extraction Techniques                                                   |
| --------------------- | ---------------------------------------------- | ----------------------------------------------------------------------- |
| **Stable Layer**      | Big Five traits, identity archetype            | Public profile parsing, questionnaire-mapped heuristics, trait lexicons |
| **Growth Layer**      | Background knowledge, roles, lived experiences | Longitudinal context, historical topics, recurring opinions             |
| **Performance Layer** | Emotion, stance, tone, stylistic patterns      | Sentiment classifiers, stance detection, stylometric features           |

### 2.2 Processing Pipeline Per Source

#### üìª Podcast

* **Input**: Transcripts via Whisper + speaker segmentation

* **Extraction**:

  * Stable: Identify consistent worldview/trait (e.g., curious, cautious)
  * Growth: Topic familiarity, opinion evolution
  * Performance: Emotional tone, disagreement stance

* **Example**:

  * *Prompt*: "AI companies should be open-source"
  * *Speaker A*: nuanced caution, hesitant tone ‚Üí introversion, analytical
  * *Speaker B*: aggressive defense, confident ‚Üí assertive, disagreeable

#### üê¶ Twitter

* **Input**: Thread + user profile (bio, posts, time span)

* **Extraction**:

  * Stable: Infer traits via LIWC lexicons, stable topics
  * Growth: Events posted over time (e.g., job change, political stance shift)
  * Performance: Per-post emotion, sarcasm, lexical variation

* **Example**:

  * ‚ÄúDay 67 of startup death spiral ‚Äî no users again.‚Äù ‚Üí High openness, low optimism
  * ‚ÄúX is a scam and I‚Äôll explain why üßµ‚Äù ‚Üí Strong assertive stance

#### üé¨ TV Dialogues

* **Input**: Multi-turn dialogues + character tags

* **Extraction**:

  * Stable: Map character (e.g., ‚ÄúRoss‚Äù = neurotic, intellectual)
  * Growth: Not typically available, but season-to-season development optional
  * Performance: Line-level tone, reaction, conflict roles

* **Example**:

  * Scene: ‚ÄúYou forgot our anniversary again?!‚Äù
  * ‚Üí Performance: anger + sarcasm; Stable: high conscientiousness (violated norm)

---

## 3. üèóÔ∏è Model Training and Injection Strategy

### 3.1 Injection Strategy by Layer

| Layer           | Modeling Technique                             | Notes                                                                      |
| --------------- | ---------------------------------------------- | -------------------------------------------------------------------------- |
| **Stable**      | Embedding initialization, system prompt + LoRA | Use public trait‚Äìtext paired datasets (e.g., PersonaChat, Big Five corpus) |
| **Growth**      | Memory module / key-value context slots        | Long-term context encoders, factual chaining                               |
| **Performance** | Prompt conditioning + decoder steering         | Stance-tuned contrastive loss or RLHF for expression                       |

### 3.2 Tools & Models

* Models: `gpt-oss-20b`, `qwen3-8b-instruct`, `llama-3.1-8b-instruct`
* Tools:

  * Trait lexicons (LIWC, Empath)
  * Persona datasets (Persona-SEE, Big Five corpus)
  * Style conditioning (CTRL, PASTEL, StylePTB)

---

## 4. üß™ Evaluation Design

### 4.1 Evaluation Modalities by Data Type

| Source          | Generation Task                                                         | Evaluation Dimensions                                  |
| --------------- | ----------------------------------------------------------------------- | ------------------------------------------------------ |
| **Podcast**     | Generate 3 podcast replies (agree, disagree, neutral) given event/topic | Stance diversity, tone clarity, opinion grounding      |
| **Twitter**     | Generate thread or response tweet with same persona                     | Consistency in language, humor, stance                 |
| **TV Dialogue** | Generate next line in character style                                   | Dialogue coherence, character adherence, emotional fit |

### 4.2 Metrics (Automatic + Human)

| Metric Category          | Example Metrics                               |
| ------------------------ | --------------------------------------------- |
| **Style Consistency**    | BLEU, BERTScore vs historical persona threads |
| **Trait Alignment**      | Trait prediction classifier on output text    |
| **Expression Diversity** | Entropy of lexical/stylistic variants         |
| **Human Evaluation**     | ‚ÄúRate alignment to original speaker (1‚Äì5)‚Äù    |

---

## 5. ‚úÖ Task List and Milestones

| Phase                         | Task                                                  | Est. Time     |
| ----------------------------- | ----------------------------------------------------- | ------------- |
| **Phase 1: Setup**            | Collect 3 corpora (podcast, Twitter, TV)              | 2 weeks       |
|                               | Implement preprocessing pipeline with annotation      | 1 week        |
| **Phase 2: Trait Extraction** | Build personality parser per layer                    | 2 weeks       |
|                               | Annotate samples + validate trait labels              | 1 week        |
| **Phase 3: Model Prep**       | Fine-tune base model with SFT + LoRA                  | 2 weeks/model |
|                               | Integrate memory/context for growth layer             | 1 week        |
| **Phase 4: Evaluation**       | Create generation benchmarks (inputs + gold examples) | 1 week        |
|                               | Run automatic metrics + human eval                    | 2 weeks       |
| **Phase 5: Report**           | Write analysis + visualize layer effects              | 1 week        |

